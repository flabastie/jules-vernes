{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d94525b",
   "metadata": {},
   "source": [
    "# Data preprocessing Jules-Verne Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf19698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655d2981",
   "metadata": {},
   "source": [
    "### Get raw data & remove heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5c0023e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Function tokenizer_punctuation\n",
    "# ------------------------------\n",
    "\n",
    "def get_and_split(data_path, limit_str):\n",
    "    # open file\n",
    "    with open(data_path, \"r\") as file:\n",
    "        contents = file.read()\n",
    "    # split contents string & return 2nd element\n",
    "    data_content = contents.split(limit_str)\n",
    "    return(data_content[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fca26d9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_ballon = get_and_split('data/01_ballon.txt', 'DEBUT DU FICHIER ballon1 --------------------------------')\n",
    "data_begum = get_and_split('data/02_begum.txt', 'DEBUT DU FICHIER begum2 --------------------------------')\n",
    "data_blocus = get_and_split('data/03_blocus.txt', 'DEBUT DU FICHIER blocus3 --------------------------------')\n",
    "data_bounty = get_and_split('data/04_bounty.txt', 'DEBUT DU FICHIER bounty1 --------------------------------')\n",
    "data_robur = get_and_split('data/05_robur.txt', 'DEBUT DU FICHIER robur1 --------------------------------')\n",
    "data_tdm80 = get_and_split('data/06_tdm80.txt', 'DEBUT DU FICHIER tdm80j2 --------------------------------')\n",
    "data_terrelune = get_and_split('data/07_terrelune.txt', 'DEBUT DU FICHIER tlun3 --------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "211b173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to check document\n",
    "# print(data_ballon)\n",
    "# print(data_begum)\n",
    "# print(data_blocus)\n",
    "# print(data_bounty)\n",
    "# print(data_robur)\n",
    "# print(data_tdm80)\n",
    "# print(data_terrelune)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24072821",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "700aacd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Function tokenizer_punctuation\n",
    "# ------------------------------\n",
    "\n",
    "def tokenizer_punctuation(sample_text):\n",
    "    # tokenizer definition\n",
    "    tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "    # return text without punctuation\n",
    "    return tokenizer.tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e78599c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_ballon = tokenizer_punctuation(data_ballon.lower())\n",
    "tokens_begum = tokenizer_punctuation(data_begum.lower())\n",
    "tokens_blocus = tokenizer_punctuation(data_blocus.lower())\n",
    "tokens_bounty = tokenizer_punctuation(data_bounty.lower())\n",
    "tokens_robur = tokenizer_punctuation(data_robur.lower())\n",
    "tokens_tdm80 = tokenizer_punctuation(data_tdm80.lower())\n",
    "tokens_terrelune = tokenizer_punctuation(data_terrelune.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c0ec7f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jules', 'verne', 'cinq', 'semaines', 'en', 'ballon', 'voyage', 'de', 'd√©couvertes', 'en']\n"
     ]
    }
   ],
   "source": [
    "# uncomment to check document\n",
    "print(tokens_ballon[:10])\n",
    "# print(tokens_begum[:10])\n",
    "# print(tokens_blocus[:10])\n",
    "# print(tokens_bounty[:10])\n",
    "# print(tokens_robur[:10])\n",
    "# print(tokens_tdm80[:10])\n",
    "# print(tokens_terrelune[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f0a5ce42",
   "metadata": {},
   "outputs": [],
   "source": [
    "french_stopwords = set(stopwords.words('french'))\n",
    "filtre_stopfr =  lambda text: [token for token in text if token.lower() not in french_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f8b76b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "swfree_ballon = filtre_stopfr(tokens_ballon)\n",
    "swfree_begum = filtre_stopfr(tokens_begum)\n",
    "swfree_blocus = filtre_stopfr(tokens_blocus)\n",
    "swfree_bounty = filtre_stopfr(tokens_bounty)\n",
    "swfree_robur = filtre_stopfr(tokens_robur)\n",
    "swfree_tdm80 = filtre_stopfr(tokens_tdm80)\n",
    "swfree_terrelune = filtre_stopfr(tokens_terrelune)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0679146",
   "metadata": {},
   "source": [
    "### Words Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "76622353",
   "metadata": {},
   "outputs": [],
   "source": [
    "freg_ballon = nltk.FreqDist(swfree_ballon) \n",
    "freg_begum = nltk.FreqDist(swfree_begum) \n",
    "freg_blocus = nltk.FreqDist(swfree_blocus) \n",
    "freg_bounty = nltk.FreqDist(swfree_bounty) \n",
    "freg_robur = nltk.FreqDist(swfree_robur) \n",
    "freg_tdm80 = nltk.FreqDist(swfree_tdm80) \n",
    "freg_terrelune = nltk.FreqDist(swfree_terrelune) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "21864d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('plus', 294),\n",
      " ('barbicane', 250),\n",
      " ('cette', 228),\n",
      " ('a', 218),\n",
      " ('mille', 198),\n",
      " ('lune', 187),\n",
      " ('cent', 170),\n",
      " ('deux', 165),\n",
      " ('si', 147),\n",
      " ('maston', 139)]\n"
     ]
    }
   ],
   "source": [
    "# uncomment to check\n",
    "# pprint.pprint(freg_ballon.most_common(10))\n",
    "# pprint.pprint(freg_begum.most_common(10))\n",
    "# pprint.pprint(freg_blocus.most_common(10))\n",
    "# pprint.pprint(freg_bounty.most_common(10))\n",
    "# pprint.pprint(freg_robur.most_common(10))\n",
    "# pprint.pprint(freg_tdm80.most_common(10))\n",
    "pprint.pprint(freg_terrelune.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29322ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
